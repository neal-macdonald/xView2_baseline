{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ccb9b6-135b-4b51-abb8-1565a169203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import ast\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801de546-83e1-4150-9e1c-40988f6c408f",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57337a2-cd93-4958-9d0b-eaabed47d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 120\n",
    "LEARNING_RATE = 0.0001\n",
    "RANDOM_SEED = 123\n",
    "LOG_STEP = 150\n",
    "\n",
    "damage_intensity_encoding = defaultdict(lambda: 0)\n",
    "damage_intensity_encoding['destroyed'] = 3\n",
    "damage_intensity_encoding['major-damage'] = 2\n",
    "damage_intensity_encoding['minor-damage'] = 1\n",
    "damage_intensity_encoding['no-damage'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47e6ebb-25b1-4a13-a51e-79a5105c7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img_array, polygon_pts, scale_pct):\n",
    "    height, width, _ = img_array.shape\n",
    "\n",
    "    xcoords = polygon_pts[:, 0]\n",
    "    ycoords = polygon_pts[:, 1]\n",
    "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
    "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
    "\n",
    "    xdiff = xmax - xmin\n",
    "    ydiff = ymax - ymin\n",
    "\n",
    "    #Extend image by scale percentage\n",
    "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
    "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
    "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
    "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
    "\n",
    "    return img_array[ymin:ymax, xmin:xmax, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06a2ac5-a80b-46a1-9880-1f568360083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path, output_path, output_csv_path, val_split_pct):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    disasters = [folder for folder in os.listdir(input_path) if not folder.startswith('.')]\n",
    "    disaster_paths = ([input_path + \"/\" +  d + \"/images\" for d in disasters])\n",
    "    image_paths = []\n",
    "    image_paths.extend([(disaster_path + \"/\" + pic) for pic in os.listdir(disaster_path)] for disaster_path in disaster_paths)\n",
    "    img_paths = np.concatenate(image_paths)\n",
    "\n",
    "    for img_path in tqdm(img_paths):\n",
    "\n",
    "        img_obj = Image.open(img_path)\n",
    "        img_array = np.array(img_obj)\n",
    "\n",
    "        #Get corresponding label for the current image\n",
    "        label_path = img_path.replace('png', 'json').replace('images', 'labels')\n",
    "        label_file = open(label_path)\n",
    "        label_data = json.load(label_file)\n",
    "\n",
    "        for feat in label_data['features']['xy']:\n",
    "\n",
    "            # only images post-disaster will have damage type\n",
    "            try:\n",
    "                damage_type = feat['properties']['subtype']\n",
    "            except: # pre-disaster damage is default no-damage\n",
    "                damage_type = \"no-damage\"\n",
    "                continue\n",
    "\n",
    "            poly_uuid = feat['properties']['uid'] + \".png\"\n",
    "\n",
    "            y_data.append(damage_intensity_encoding[damage_type])\n",
    "\n",
    "            polygon_geom = shapely.wkt.loads(feat['wkt'])\n",
    "            polygon_pts = np.array(list(polygon_geom.exterior.coords))\n",
    "            poly_img = process_img(img_array, polygon_pts, 0.8)\n",
    "            cv2.imwrite(output_path + \"/\" + poly_uuid, poly_img)\n",
    "            x_data.append(poly_uuid)\n",
    "    \n",
    "    output_train_csv_path = os.path.join(output_csv_path, \"train.csv\")\n",
    "\n",
    "    if(val_split_pct > 0):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=val_split_pct)\n",
    "        data_array_train = {'uuid': x_train, 'labels': y_train}\n",
    "        data_array_test = {'uuid': x_test, 'labels': y_test}\n",
    "        output_test_csv_path = os.path.join(output_csv_path, \"test.csv\")\n",
    "        df_train = pd.DataFrame(data_array_train)\n",
    "        df_test = pd.DataFrame(data_array_test)\n",
    "        df_train.to_csv(output_train_csv_path)\n",
    "        df_test.to_csv(output_test_csv_path)\n",
    "    else: \n",
    "        data_array = {'uuid': x_data, 'labels': y_data}\n",
    "        df = pd.DataFrame(data = data_array)\n",
    "        df.to_csv(output_train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97008c2-c1a5-4a41-8e72-c0b700f5cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"\"\n",
    "output_dir = r\"\"\n",
    "output_dir_csv = r\"\"\n",
    "val_split_pct = 0.0\n",
    "\n",
    "process_data(input_dir, output_dir, output_dir_csv, float(val_split_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fce47b-f5bb-4b39-8dae-cbd58bd5fe48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f856740c-5b80-4618-9276-6672af52caac",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cdb231-3a03-4e95-944c-558d3b92d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_loss(y_true, y_pred):\n",
    "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
    "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43198e5f-318c-42df-a291-30be50adc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xBD_baseline_model():\n",
    "    weights = 'imagenet'\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "    base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=(128, 128, 3))(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    base_resnet = base_model(inputs)\n",
    "    base_resnet = Flatten()(base_resnet)\n",
    "\n",
    "    concated_layers = Concatenate()([x, base_resnet])\n",
    "\n",
    "    concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
    "    concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "    concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "    output = Dense(4, activation='relu')(concated_layers)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
