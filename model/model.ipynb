{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ccb9b6-135b-4b51-abb8-1565a169203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tqdm) (0.4.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from opencv-python) (1.23.1)\n",
      "Requirement already satisfied: shapely in c:\\users\\namacdon\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (1.8.2)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "import sys,os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "!pip install shapely\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import ast\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801de546-83e1-4150-9e1c-40988f6c408f",
   "metadata": {},
   "source": [
    "#### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57337a2-cd93-4958-9d0b-eaabed47d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 16 #64\n",
    "NUM_EPOCHS = 10 #120\n",
    "LEARNING_RATE = 0.0001\n",
    "RANDOM_SEED = 123\n",
    "LOG_STEP = 150\n",
    "\n",
    "damage_intensity_encoding = defaultdict(lambda: 0)\n",
    "damage_intensity_encoding['destroyed'] = 3\n",
    "damage_intensity_encoding['major-damage'] = 2\n",
    "damage_intensity_encoding['minor-damage'] = 1\n",
    "damage_intensity_encoding['no-damage'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47e6ebb-25b1-4a13-a51e-79a5105c7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img_array, polygon_pts, scale_pct):\n",
    "    height, width, _ = img_array.shape\n",
    "\n",
    "    xcoords = polygon_pts[:, 0]\n",
    "    ycoords = polygon_pts[:, 1]\n",
    "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
    "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
    "\n",
    "    xdiff = xmax - xmin\n",
    "    ydiff = ymax - ymin\n",
    "\n",
    "    #Extend image by scale percentage\n",
    "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
    "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
    "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
    "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
    "\n",
    "    return img_array[ymin:ymax, xmin:xmax, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06a2ac5-a80b-46a1-9880-1f568360083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path, output_path, output_csv_path, val_split_pct):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    disasters = [folder for folder in os.listdir(input_path) if not folder.startswith('.')]\n",
    "    disaster_paths = ([input_path + f\"\\\\{d}\\\\images\" for d in disasters])\n",
    "    image_paths = []\n",
    "    image_paths.extend([(disaster_path + \"\\\\\" + pic) for pic in os.listdir(disaster_path)] for disaster_path in disaster_paths)\n",
    "    img_paths = np.concatenate(image_paths)\n",
    "    \n",
    "    \n",
    "    for img_path in tqdm(img_paths):\n",
    "\n",
    "        if os.path.join(os.path.dirname(os.path.dirname(img_path)),\"labels\") != os.getcwd():\n",
    "            os.chdir(os.path.join(os.path.dirname(os.path.dirname(img_path)),\"labels\"))\n",
    "        \n",
    "        img_obj = Image.open(img_path)\n",
    "        img_array = np.array(img_obj)\n",
    "        basename = os.path.basename(img_path)\n",
    "        \n",
    "        #Get corresponding label for the current image\n",
    "        label_path = basename.replace('png', 'json')\n",
    "        label_file = open(label_path)\n",
    "        label_data = json.load(label_file)\n",
    "\n",
    "        for feat in label_data['features']['xy']:\n",
    "\n",
    "            # only images post-disaster will have damage type\n",
    "            try:\n",
    "                damage_type = feat['properties']['subtype']\n",
    "            except: # pre-disaster damage is default no-damage\n",
    "                damage_type = \"no-damage\"\n",
    "                continue\n",
    "\n",
    "            poly_uuid = feat['properties']['uid'] + \".png\"\n",
    "\n",
    "            y_data.append(damage_intensity_encoding[damage_type])\n",
    "\n",
    "            polygon_geom = shapely.wkt.loads(feat['wkt'])\n",
    "            polygon_pts = np.array(list(polygon_geom.exterior.coords))\n",
    "            poly_img = process_img(img_array, polygon_pts, 0.8)\n",
    "            cv2.imwrite(output_path + \"/\" + poly_uuid, poly_img)\n",
    "            x_data.append(poly_uuid)\n",
    "    \n",
    "    output_train_csv_path = os.path.join(output_csv_path, \"train.csv\")\n",
    "\n",
    "    if(val_split_pct > 0):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=val_split_pct)\n",
    "        data_array_train = {'uuid': x_train, 'labels': y_train}\n",
    "        data_array_test = {'uuid': x_test, 'labels': y_test}\n",
    "        output_test_csv_path = os.path.join(output_csv_path, \"test.csv\")\n",
    "        df_train = pd.DataFrame(data_array_train)\n",
    "        df_test = pd.DataFrame(data_array_test)\n",
    "        df_train.to_csv(output_train_csv_path)\n",
    "        df_test.to_csv(output_test_csv_path)\n",
    "    else: \n",
    "        data_array = {'uuid': x_data, 'labels': y_data}\n",
    "        df = pd.DataFrame(data = data_array)\n",
    "        df.to_csv(output_train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97008c2-c1a5-4a41-8e72-c0b700f5cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"C:\\Users\\namacdon\\Desktop\\delete\\xView2\\train_images_labels_targets\\disasters\"\n",
    "output_dir = r\"C:\\Users\\namacdon\\Desktop\\delete\\xView2\\train_images_labels_targets\\output\"\n",
    "output_dir_csv = r\"C:\\Users\\namacdon\\Desktop\\delete\\xView2\\train_images_labels_targets\\output\\output.csv\"\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "else:\n",
    "    print(\"Output Path Already Exists\")\n",
    "    sys.exit(1)\n",
    "if not os.path.exists(output_dir_csv):\n",
    "    with open(output_dir_csv, 'w') as my_new_csv_file:\n",
    "        pass\n",
    "else:\n",
    "    print(\"Output CSV Path Already Exists\")\n",
    "    sys.exit(1)\n",
    "val_split_pct = 0.0\n",
    "\n",
    "process_data(input_dir, output_dir, output_dir_csv, float(val_split_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856740c-5b80-4618-9276-6672af52caac",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cdb231-3a03-4e95-944c-558d3b92d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_loss(y_true, y_pred):\n",
    "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
    "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43198e5f-318c-42df-a291-30be50adc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xBD_baseline_model():\n",
    "    weights = None\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "    base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=(128, 128, 3))(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    base_resnet = base_model(inputs)\n",
    "    base_resnet = Flatten()(base_resnet)\n",
    "\n",
    "    concated_layers = Concatenate()([x, base_resnet])\n",
    "\n",
    "    concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
    "    concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "    concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "    output = Dense(4, activation='relu')(concated_layers)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bf852-2eaf-494d-aad2-2482b8b98255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_xBD_baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfa1be-9c38-4dc4-bcb2-973e4fec04ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a42f8-9ec6-458f-b730-4c9b925431d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c2e5b-ea8f-49e8-8fc2-5ceed62b68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_generator(test_csv, test_dir):\n",
    "    df = pd.read_csv(test_csv)\n",
    "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
    "\n",
    "    gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                             rescale=1/255.)\n",
    "\n",
    "\n",
    "    return gen.flow_from_dataframe(dataframe=df,\n",
    "                                   directory=test_dir,\n",
    "                                   x_col='uuid',\n",
    "                                   y_col='labels',\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   shuffle=False,\n",
    "                                   seed=RANDOM_SEED,\n",
    "                                   class_mode=\"categorical\",\n",
    "                                   target_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee1940-e66d-4320-a289-f5ebff4fafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df, in_dir):\n",
    "\n",
    "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
    "    gen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             rescale=1/255.)\n",
    "    return gen.flow_from_dataframe(dataframe=df,\n",
    "                                   directory=in_dir,\n",
    "                                   x_col='uuid',\n",
    "                                   y_col='labels',\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   seed=RANDOM_SEED,\n",
    "                                   class_mode=\"categorical\",\n",
    "                                   target_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb98170-dce4-415c-a3fd-0c9aed707857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, train_csv, test_data, test_csv, model_in, model_out):\n",
    "\n",
    "    model = generate_xBD_baseline_model()\n",
    "\n",
    "    # Add model weights if provided by user\n",
    "    if model_in is not None:\n",
    "        model.load_weights(model_in)\n",
    "\n",
    "    df = pd.read_csv(train_csv)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(df['labels'].to_list()), df['labels'].to_list());\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    samples = df['uuid'].count()\n",
    "    steps = np.ceil(samples/BATCH_SIZE)\n",
    "\n",
    "    # Augments the training data\n",
    "    train_gen_flow = augment_data(df, train_data)\n",
    "\n",
    "    #Set up tensorboard logging\n",
    "    tensorboard_callbacks = keras.callbacks.TensorBoard(log_dir=LOG_DIR,\n",
    "                                                        batch_size=BATCH_SIZE)\n",
    "\n",
    "    \n",
    "    #Filepath to save model weights\n",
    "    filepath = model_out + \"-saved-model-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "    checkpoints = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                    monitor=['loss', 'accuracy'],\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=False,\n",
    "                                                    mode='max')\n",
    "\n",
    "    #Adds adam optimizer\n",
    "    adam = keras.optimizers.Adam(lr=LEARNING_RATE,\n",
    "                                    beta_1=0.9,\n",
    "                                    beta_2=0.999,\n",
    "                                    decay=0.0,\n",
    "                                    amsgrad=False)\n",
    "\n",
    "\n",
    "    model.compile(loss=ordinal_loss, optimizer=adam, metrics=['accuracy', f1])\n",
    "\n",
    "    #Training begins\n",
    "    model.fit_generator(generator=train_gen_flow,\n",
    "                        steps_per_epoch=steps,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        workers=NUM_WORKERS,\n",
    "                        use_multiprocessing=True,\n",
    "                        class_weight=d_class_weights,\n",
    "                        callbacks=[tensorboard_callbacks, checkpoints],\n",
    "                        verbose=1)\n",
    "\n",
    "\n",
    "    #Evalulate f1 weighted scores on validation set\n",
    "    validation_gen = validation_generator(test_csv, test_data)\n",
    "    predictions = model.predict(validation_gen)\n",
    "\n",
    "    val_trues = validation_gen.classes\n",
    "    val_pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    f1_weighted = f1_score(val_trues, val_pred, average='weighted')\n",
    "    print(f1_weighted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
