{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db215741-a8ef-448e-b8be-3829cf05eb98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6516/2206961899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinks\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chainer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "from unet import UNet\n",
    "from dataset import LabeledImageDataset\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8126c-6b4b-44f5-b787-12b7d87f1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('dataset', help='Path to directory containing train.txt, val.txt, and mean.npy')\n",
    "    parser.add_argument('images',  help='Root directory of input images')\n",
    "    parser.add_argument('labels',  help='Root directory of label images')\n",
    "    \n",
    "    parser.add_argument('--batchsize', '-b', type=int, default=16,\n",
    "                        help='Number of images in each mini-batch')\n",
    "    parser.add_argument('--test-batchsize', '-B', type=int, default=4,\n",
    "                        help='Number of images in each test mini-batch')\n",
    "    parser.add_argument('--epoch', '-e', type=int, default=50,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--frequency', '-f', type=int, default=1,\n",
    "                        help='Frequency of taking a snapshot')\n",
    "    parser.add_argument('--gpu', '-g', type=int, default=0,\n",
    "                        help='GPU ID (negative value indicates CPU)')\n",
    "    parser.add_argument('--out', '-o', default='logs',\n",
    "                        help='Directory to output the result under \"models\" directory')\n",
    "    parser.add_argument('--resume', '-r', default='',\n",
    "                        help='Resume the training from snapshot')\n",
    "    parser.add_argument('--noplot', dest='plot', action='store_false',\n",
    "                        help='Disable PlotReport extension')\n",
    "\n",
    "    parser.add_argument('--tcrop', type=int, default=400,\n",
    "                        help='Crop size for train-set images')\n",
    "    parser.add_argument('--vcrop', type=int, default=480,\n",
    "                        help='Crop size for validation-set images')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    assert (args.tcrop % 16 == 0) and (args.vcrop % 16 == 0), \"tcrop and vcrop must be divisible by 16.\"\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        from tboard_logger_cpu import TensorboardLogger\n",
    "    else:\n",
    "        from tboard_logger import TensorboardLogger\n",
    "\n",
    "    print('GPU: {}'.format(args.gpu))\n",
    "    print('# Minibatch-size: {}'.format(args.batchsize))\n",
    "    print('# Crop-size: {}'.format(args.tcrop))\n",
    "    print('# epoch: {}'.format(args.epoch))\n",
    "    print('')\n",
    "    \n",
    "    this_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    models_dir = os.path.normpath(os.path.join(this_dir, \"../../models\"))\n",
    "    log_dir = os.path.join(models_dir, args.out)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    # Set up a neural network to train\n",
    "    # Classifier reports softmax cross entropy loss and accuracy at every\n",
    "    # iteration, which will be used by the PrintReport extension below.\n",
    "    model = UNet()\n",
    "    if args.gpu >= 0:\n",
    "        # Make a specified GPU current\n",
    "        chainer.cuda.get_device_from_id(args.gpu).use()\n",
    "        model.to_gpu()  # Copy the model to the GPU\n",
    "\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    # Load mean image\n",
    "    mean = np.load(os.path.join(args.dataset, \"mean.npy\"))\n",
    "    \n",
    "    # Load the MNIST dataset\n",
    "    train = LabeledImageDataset(os.path.join(args.dataset, \"train.txt\"), args.images, args.labels, \n",
    "                                mean=mean, crop_size=args.tcrop, test=False, distort=False)\n",
    "    \n",
    "    test = LabeledImageDataset (os.path.join(args.dataset, \"val.txt\"), args.images, args.labels, \n",
    "                                mean=mean, crop_size=args.vcrop, test=True, distort=False)\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, args.test_batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = training.StandardUpdater(\n",
    "        train_iter, optimizer, device=args.gpu)\n",
    "    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=log_dir)\n",
    "\n",
    "    # Evaluate the model with the test dataset for each epoch\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=args.gpu))\n",
    "\n",
    "    # Dump a computational graph from 'loss' variable at the first iteration\n",
    "    # The \"main\" refers to the target link of the \"main\" optimizer.\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "    # Take a snapshot for each specified epoch\n",
    "    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
    "    trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
    "    \n",
    "    # Save trained model for each specific epoch\n",
    "    trainer.extend(extensions.snapshot_object(\n",
    "        model, 'model_iter_{.updater.iteration}'), trigger=(frequency, 'epoch'))\n",
    "\n",
    "    # Write a log of evaluation statistics for each epoch\n",
    "    trainer.extend(extensions.LogReport())\n",
    "\n",
    "    # Save two plot images to the result dir\n",
    "    if args.plot and extensions.PlotReport.available():\n",
    "        trainer.extend(\n",
    "            extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                                  'epoch', file_name='loss.png'))\n",
    "        trainer.extend(\n",
    "            extensions.PlotReport(\n",
    "                ['main/accuracy', 'validation/main/accuracy'],\n",
    "                'epoch', file_name='accuracy.png'))\n",
    "\n",
    "    # Print selected entries of the log to stdout\n",
    "    # Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "    # \"validation\" refers to the default name of the Evaluator extension.\n",
    "    # Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "    # either the updater or the evaluator.\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "    # Print a progress bar to stdout\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "    \n",
    "    # Write training log to TensorBoard log file\n",
    "    trainer.extend(TensorboardLogger(writer,\n",
    "        ['main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy']))\n",
    "    \n",
    "    if args.resume:\n",
    "        # Resume from a snapshot\n",
    "        chainer.serializers.load_npz(args.resume, trainer)\n",
    "\n",
    "    # Run the training\n",
    "    trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
